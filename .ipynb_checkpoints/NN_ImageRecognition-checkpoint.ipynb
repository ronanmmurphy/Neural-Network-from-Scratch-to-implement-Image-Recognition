{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0auBNhQ7wVP5"
   },
   "source": [
    "# Deep Learning: NN from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53z33mKDwVP7"
   },
   "source": [
    "# Part 1:\n",
    "Create one hidden layer Neural Network Model from scratch. This model is feed-forward and includes back-propagation. Created generic initalise method to handle multiple input, output and hidden nodes defined by the input data (the hidden nodes is a constant set before training). The training data, after the nodes weights are initalised randomly, is fed-forward updating the weights with the inputted data and the sum is used in the activaiton function tanh(x) which calculates the output of that node. The error of the each forward propagtion is calcualted comparing the expected vs actual output to give an error. Stochastic Gradient descent is implemented to update the weights by the learning rate and the error in back propagation to improve the output for each epoch. Once the model is trained the test data is forward propagated through the model to get a classification prediction. Sckiet learn accuracy compares the actual test labels with the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTuknkWWwVP9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import random\n",
    "import math\n",
    "\n",
    "#function to create initial Neural Network \n",
    "#takes in parameters for inputs, hidden nodes and outputs\n",
    "def init_NN(nodes_in, nodes_h, nodes_out):\n",
    "    \n",
    "    #create list for all initalised nodes and their weights\n",
    "    init = []\n",
    "    #create list for hidden nodes\n",
    "    layer_h = []\n",
    "    #create list for output nodes\n",
    "    layer_out = []\n",
    "    \n",
    "    #for all nodes in hidden append random weight to dict for each input node +1\n",
    "    #there will be extra input weight for the bias\n",
    "    for i in range(nodes_h):\n",
    "        l = []\n",
    "        for j in range(nodes_in+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_h.append(dict_weights)\n",
    "        \n",
    "    #for all nodes in output append random weight to dict for each hidden node +1\n",
    "    #there will be extra hidden weight for the bias\n",
    "    for i in range(nodes_out):\n",
    "        l = []\n",
    "        for j in range(nodes_h+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_out.append(dict_weights)\n",
    "        \n",
    "    #append both hidden and output lists to initalised Neural Network\n",
    "    init.append(layer_h)\n",
    "    init.append(layer_out)\n",
    "    return init\n",
    "\n",
    "\n",
    "#tanh activiate weights function\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "#tanh derivative activate weights function for gradiant descent \n",
    "\n",
    "def grad_tanh(x):\n",
    "    return (1-np.square(x))\n",
    "\n",
    "#function to forward propagate through network\n",
    "\n",
    "def forward_prop(init, inputs):\n",
    "    for layer in init:\n",
    "        #new list to update weights\n",
    "        updated_weights = []\n",
    "        \n",
    "        for node in layer:\n",
    "            #get the weights from the dictionary for eachnode\n",
    "            weights = node['w']\n",
    "            #inialised to last weight in dict\n",
    "            initalise = weights[-1]\n",
    "            #add new weights  = old weights  * input data\n",
    "            for i in range(len(weights)-1):\n",
    "                initalise += weights[i] * inputs[i]\n",
    "            #use tanh activation on new weights as non-linear model\n",
    "            node['o'] = tanh(initalise)\n",
    "            #output this as inputs for next layer\n",
    "            updated_weights.append(node['o'])\n",
    "        inputs = updated_weights\n",
    "    return inputs\n",
    "\n",
    "#cost function to determine the error of each node for gradient descent\n",
    "\n",
    "def cost(init, output):\n",
    "    for row in reversed(range(len(init))):\n",
    "        layer = init[row]\n",
    "        cost = []\n",
    "        #for nodes except the output find the cost of that node\n",
    "        if row != len(init)-1:\n",
    "            for cell in range(len(layer)):\n",
    "                #inialise cost to zero\n",
    "                node_cost = 0.0\n",
    "                #get the cost of each node in layer by finding error\n",
    "                #multiply delta value of node with the weight of that node\n",
    "                for node in init[row + 1]:\n",
    "                    node_cost += (node['w'][cell] * node['d'])\n",
    "                cost.append(node_cost)\n",
    "        else:\n",
    "            #for output node cost is predicted output - actual value\n",
    "            for cell in range(len(layer)):\n",
    "                node = layer[cell]\n",
    "                cost.append(output[cell] - node['o'])\n",
    "        for cell in range(len(layer)):\n",
    "            #get the cost for each node node multiplying error by the derivative of activation\n",
    "            #tanh on the output node\n",
    "            #assign this to delta in node which will be used to update weights\n",
    "            node = layer[cell]\n",
    "            node['d'] = cost[cell] * grad_tanh(node['o'])\n",
    "            \n",
    "#back propagte through the neural network updating the weights with SGD\n",
    "\n",
    "def back_prop(init, data, learning, epoch, output):\n",
    "    #repeat feed forward and backwards for each epoch \n",
    "    for e in range(epoch):\n",
    "        epoch_error = 0\n",
    "        for inputs in data:\n",
    "            #for every row in training set call forward propagate \n",
    "            result = forward_prop(init, inputs)\n",
    "            #give the label ouputs as parameters for find the cost of nodes\n",
    "            outputs = [0 for out in range(output)]\n",
    "            outputs[inputs[-1]] = 1\n",
    "            cost(init, outputs)\n",
    "            epoch_error += sum([(outputs[out]-result[out])**2 for out in range(len(outputs))])\n",
    "            #update weights based on new cost\n",
    "            for layer in range(len(init)):\n",
    "                update_input = inputs[:-1]\n",
    "                #for all layers add output result to list\n",
    "                if layer !=0:\n",
    "                    update_input = []\n",
    "                    for node in init[layer-1]:\n",
    "                        update_input.append(node['o'])\n",
    "                #for every node in each layer change the weights based on formula SGD\n",
    "                # ( learning rate * cost of error of that node * the output of node )\n",
    "                for node in init[layer]:\n",
    "                    for cell in range(len(update_input)):\n",
    "                        node['w'][cell] += learning*node['d']*update_input[cell]\n",
    "        print(\"epochs: \", e,\" learning rate: \", learning, \"error: \",epoch_error)\n",
    "                        \n",
    "#method to predict the y values of each row in test returning max from forward propagation of the model  \n",
    "\n",
    "def predict(init, row):\n",
    "    out = forward_prop(init, row)\n",
    "    return out.index(max(out))\n",
    "\n",
    "#method to train and test the Neural Network\n",
    "\n",
    "def train_NN(train, test, learning, epoch, hidden):\n",
    "    #set the amount of inputs to NN by the length of columns for parameters\n",
    "    inputs = len(train[0]) - 1\n",
    "    #set amount of outputs to all values outputted ie. 0/1\n",
    "    outputs = len(set([row[-1] for row in train]))\n",
    "    #call init_NN function to set amount of nodes in each layer and their weights\n",
    "    init = init_NN(inputs, hidden,outputs)\n",
    "    #call back propagate function to train model\n",
    "    back_prop(init, train, learning, epoch, outputs)\n",
    "    #create list of predictions for each row of test data\n",
    "    y_pred = []\n",
    "    for row in test:\n",
    "        row_predict = predict(init, row)\n",
    "        y_pred.append(row_predict)\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5po72loGwVQA"
   },
   "source": [
    "# Part 2:\n",
    "Test the neural network on simple CSV file circles500.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use panads to read csv skipping first line column names\n",
    "df = pd.read_csv('circles500.csv', header = None, skiprows=1)\n",
    "\n",
    "#scale values from mix to max to normalise results\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(df))\n",
    "\n",
    "#split the data into training and test with 60-40% split most effective\n",
    "train, test = train_test_split(df, test_size=0.34)\n",
    "\n",
    "#set learning rate, number of epochs and number of hidden layers in model\n",
    "learning = 0.1\n",
    "epoch = 1000\n",
    "hidden_nodes = 5\n",
    "\n",
    "#convert the dataframes to list \n",
    "train =train.values.tolist()\n",
    "test = test.values.tolist()\n",
    "\n",
    "#for the training and test set convert the label of each row to an integer from a float\n",
    "for x in range(len(train)):\n",
    "    train[x][-1] = int(train[x][-1]) \n",
    "\n",
    "for y in range(len(test)):\n",
    "    test[y][-1] = int(test[y][-1])\n",
    "\n",
    "#get the actual output label from the test set\n",
    "y_test = []\n",
    "for row in range(len(test)):\n",
    "    y_test.append(test[row][-1])\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test)):\n",
    "    #assign label to None\n",
    "    test[row][-1] = None\n",
    "#get the predicted labels\n",
    "y_pred = train_NN(train, test, learning, epoch, hidden_nodes)\n",
    "\n",
    "#get the accuracy of the model\n",
    "pred_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Prediction Accuracy = \", pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9K1rBEFwVQF"
   },
   "source": [
    "# Part 3:\n",
    "Test the Neural Network on a more difficult dataset comparing frogs and deers as given. This data set proved very slow to run and was taking approx 10mins per epoch we werent able to figure out the problem and therefore the results are lower than they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data_dict = unpickle(\"data_batch_1\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data1 = data_dict[b'data']\n",
    "\n",
    "\"\"\" This function converts the raw array of 3072 floats describing each image into a structure where the colour of each pixel\n",
    "in an image is represented by its r,g,b (in this order) value for every pixel in the image in question.\n",
    "The input into the function is an array of 3072 floats.\n",
    "This assumes that the raw data coming from the CIFAR data is in the format where all the r values come first, followed by\n",
    "all the b values and then all the g values \"\"\"\n",
    "def convert_rgb(dataplusindex):\n",
    "    rgb_picture = dataplusindex\n",
    "    rgb_picture.shape = (3,32,32)\n",
    "    rgb_picture = rgb_picture.transpose([1, 2, 0])\n",
    "   \n",
    "    return rgb_picture\n",
    "\n",
    "\n",
    "\"\"\"This function converts the R,G,B pixels into greyscale pixels. There are a few methods for doing this.\n",
    "If the data is ordered correctly as R,G,B it is appropriate to use Method 2 (link below) which multiplies each component colour by\n",
    "a certain constant, and then sums the total.\n",
    "If not, one can use a standard average of the R,G,B, Method 1 (link below)\"\"\"    \n",
    "def convert_greyscale(rgb_pic):\n",
    "    greyscale_array = []\n",
    "    for element in rgb_pic:\n",
    "        for rgb_row in element:\n",
    "            x = round(np.dot(rgb_row,[0.299, 0.587, 0.114]), 2) #https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 2, assuming the rgb values come in in the format r, g, b and not b,g,r for example\n",
    "            #x = (rgb_row[0] + rgb_row[1] + rgb_row[2])/3 # https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 1\n",
    "            greyscale_array.append(x)\n",
    "    greyscale_array = np.array(greyscale_array)\n",
    "    return greyscale_array\n",
    "\n",
    "# Converting the data into format [greyscale list, classification].\n",
    "# Each data instance will be added to a larger list.\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list = []\n",
    "for i in range(len(data_dict[b'labels'])):\n",
    "    if data_dict[b'labels'][i] == 4 or data_dict[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr = convert_greyscale(rgb_pic)\n",
    "        if data_dict[b'labels'][i] == 4:\n",
    "            greyscale_arr = np.append(greyscale_arr,0)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "           \n",
    "        if data_dict[b'labels'][i] == 6:\n",
    "            greyscale_arr = np.append(greyscale_arr,1)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list = []\n",
    "for element in Frog_Deer_classified_list:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_train = pd.DataFrame(new_Frog_Deer_classified_list)\n",
    "#print(df_train)\n",
    "\n",
    "\n",
    "\n",
    "data_dict_test = unpickle(\"test_batch\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data_test = data_dict_test[b'data']\n",
    "\n",
    "# Converting the data into format [greyscale list, classification].\n",
    "# Each data instance will be added to a larger list.\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list_test = []\n",
    "for i in range(len(data_dict_test[b'labels'])):\n",
    "    if data_dict_test[b'labels'][i] == 4 or data_dict_test[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic_test = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr_test = convert_greyscale(rgb_pic_test)\n",
    "        if data_dict_test[b'labels'][i] == 4:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,0)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "           \n",
    "        if data_dict_test[b'labels'][i] == 6:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,1)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list_test = []\n",
    "for element in Frog_Deer_classified_list_test:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list_test.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_test = pd.DataFrame(new_Frog_Deer_classified_list_test)\n",
    "df_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set learning rate, number of epochs and number of hidden layers in model\n",
    "learning = 0.1\n",
    "epoch = 20\n",
    "hidden_nodes = 12\n",
    "\n",
    "#convert the dataframes to list \n",
    "train =df_train.values.tolist()\n",
    "test = df_test.values.tolist()\n",
    "\n",
    "#for the training and test set convert the label of each row to an integer from a float\n",
    "for x in range(len(train)):\n",
    "    train[x][-1] = int(train[x][-1]) \n",
    "\n",
    "for y in range(len(test)):\n",
    "    test[y][-1] = int(test[y][-1])\n",
    "\n",
    "#get the actual output label from the test set\n",
    "y_test = []\n",
    "for row in range(len(test)):\n",
    "    y_test.append(test[row][-1])\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test)):\n",
    "    #assign label to None\n",
    "    test[row][-1] = None\n",
    "#get the predicted labels\n",
    "y_pred1 = train_NN(train, test, learning, epoch, hidden_nodes)\n",
    "\n",
    "#get the accuracy of the model\n",
    "x_new = accuracy_score(y_test, y_pred1)\n",
    "print(\"Prediction accuracy of model\", x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RHf9o_awVQM"
   },
   "source": [
    "# Part 4a:\n",
    "\n",
    "\n",
    "Implement 1st Enhancemnt - Added NN Layer to make 2 layer Neural Network\n",
    "\n",
    "\n",
    "Part i: Test added layer on circles500.csv\n",
    "\n",
    "Part ii: Test on images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part i\n",
    "#function to create initial Neural Network \n",
    "#takes in parameters for inputs, hidden nodes and outputs\n",
    "def init_NN2(nodes_in, nodes_h, nodes_h2,nodes_out):\n",
    "    \n",
    "    #create list for all initalised nodes and their weights\n",
    "    init = []\n",
    "    #create list for hidden nodes\n",
    "    layer_h = []\n",
    "    #create list for hidden layer 2 nodes\n",
    "    layer_h2 =[]\n",
    "    #create list for output nodes\n",
    "    layer_out = []\n",
    "    \n",
    "    #for all nodes in hidden append random weight to dict for each input node +1\n",
    "    #there will be extra input weight for the bias\n",
    "    for i in range(nodes_h):\n",
    "        l = []\n",
    "        for j in range(nodes_in+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_h.append(dict_weights)\n",
    "        \n",
    "    #add extra hidden layer\n",
    "    #for all nodes in hidden append random weight to dict for each input node +1\n",
    "    #there will be extra input weight for the bias\n",
    "    for i in range(nodes_h2):\n",
    "        l = []\n",
    "        for j in range(nodes_h+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_h2.append(dict_weights)\n",
    "        \n",
    "    \n",
    "    #for all nodes in output append random weight to dict for each hidden node +1\n",
    "    #there will be extra hidden weight for the bias\n",
    "    for i in range(nodes_out):\n",
    "        l = []\n",
    "        for j in range(nodes_h2+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_out.append(dict_weights)\n",
    "        \n",
    "    #append both hidden, hidden2 and output lists to initalised Neural Network\n",
    "    init.append(layer_h)\n",
    "    init.append(layer_h2)\n",
    "    init.append(layer_out)\n",
    "    return init\n",
    "\n",
    "\n",
    "#tanh activiate weights function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "#tanh derivative activate weights function for gradiant descent \n",
    "def grad_tanh(x):\n",
    "    return (1-np.square(x))\n",
    "\n",
    "#function to forward propagate through network\n",
    "def forward_prop(init, inputs):\n",
    "    for layer in init:\n",
    "        #new list to update weights\n",
    "        updated_weights = []\n",
    "        \n",
    "        for node in layer:\n",
    "            #get the weights from the dictionary for eachnode\n",
    "            weights = node['w']\n",
    "            #inialised to last weight in dict\n",
    "            initalise = weights[-1]\n",
    "            #add new weights  = old weights  * input data\n",
    "            for i in range(len(weights)-1):\n",
    "                initalise += weights[i] * inputs[i]\n",
    "            #use tanh activation on new weights as non-linear model\n",
    "            node['o'] = tanh(initalise)\n",
    "            #output this as inputs for next layer\n",
    "            updated_weights.append(node['o'])\n",
    "        inputs = updated_weights\n",
    "    return inputs\n",
    "\n",
    "#cost function to determine the error of each node for gradient descent \n",
    "def cost(init, output):\n",
    "    for row in reversed(range(len(init))):\n",
    "        layer = init[row]\n",
    "        cost = []\n",
    "        #for nodes except the output find the cost of that node\n",
    "        if row != len(init)-1:\n",
    "            for cell in range(len(layer)):\n",
    "                #inialise cost to zero\n",
    "                node_cost = 0.0\n",
    "                #get the cost of each node in layer by finding error\n",
    "                #multiply delta value of node with the weight of that node\n",
    "                for node in init[row + 1]:\n",
    "                    node_cost += (node['w'][cell] * node['d'])\n",
    "                cost.append(node_cost)\n",
    "        else:\n",
    "            #for output node cost is predicted output - actual value\n",
    "            for cell in range(len(layer)):\n",
    "                node = layer[cell]\n",
    "                cost.append(output[cell] - node['o'])\n",
    "        for cell in range(len(layer)):\n",
    "            #get the cost for each node node multiplying error by the derivative of activation\n",
    "            #tanh on the output node\n",
    "            #assign this to delta in node which will be used to update weights\n",
    "            node = layer[cell]\n",
    "            node['d'] = cost[cell] * grad_tanh(node['o'])\n",
    "            \n",
    "#back propagte through the neural network updating the weights with SGD\n",
    "def back_prop(init, data, learning, epoch, output):\n",
    "    #repeat feed forward and backwards for each epoch \n",
    "    sum_error = 0\n",
    "    for e in range(epoch):\n",
    "        for inputs in data:\n",
    "            #for every row in training set call forward propagate \n",
    "            result = forward_prop(init, inputs)\n",
    "            #give the label ouputs as parameters for find the cost of nodes\n",
    "            outputs = [0 for out in range(output)]\n",
    "            outputs[inputs[-1]] = 1\n",
    "            cost(init, outputs)\n",
    "            \n",
    "            sum_error += sum([(outputs[out]-result[out])**2 for out in range(len(outputs))])\n",
    "            #update weights based on new cost\n",
    "            for layer in range(len(init)):\n",
    "                update_input = inputs[:-1]\n",
    "                #for all layers add output result to list\n",
    "                if layer !=0:\n",
    "                    update_input = []\n",
    "                    for node in init[layer-1]:\n",
    "                        update_input.append(node['o'])\n",
    "                #for every node in each layer change the weights based on formula SGD\n",
    "                # ( learning rate * cost of error of that node * the output of node )\n",
    "                for node in init[layer]:\n",
    "                    for cell in range(len(update_input)):\n",
    "                        node['w'][cell] += learning*node['d']*update_input[cell]\n",
    "            print(\"epochs: \", e,\" learning rate: \", learning, \"error: \",sum_error)\n",
    "\n",
    "                        \n",
    "#method to predict the y values of each row in test returning max from forward propagation of the model               \n",
    "def predict(init, row):\n",
    "    out = forward_prop(init, row)\n",
    "    return out.index(max(out))\n",
    "\n",
    "#method to train and test the Neural Network\n",
    "def train_NN2(train, test, learning, epoch, hidden, hidden2):\n",
    "    #set the amount of inputs to NN by the length of columns for parameters\n",
    "    inputs = len(train[0]) - 1\n",
    "    #set amount of outputs to all values outputted ie. 0/1\n",
    "    outputs = len(set([row[-1] for row in train]))\n",
    "    #call init_NN function to set amount of nodes in each layer and their weights\n",
    "    init = init_NN2(inputs, hidden,hidden2, outputs)\n",
    "    #call back propagate function to train model\n",
    "    back_prop(init, train, learning, epoch, outputs)\n",
    "    #create list of predictions for each row of test data\n",
    "    y_pred = []\n",
    "    for row in test:\n",
    "        row_predict = predict(init, row)\n",
    "        y_pred.append(row_predict)\n",
    "    return(y_pred)\n",
    "df = pd.read_csv('circles500.csv', header = None, skiprows=1)\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(df))\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.34)\n",
    "\n",
    "\n",
    "learning = 0.1\n",
    "epoch = 10000\n",
    "hidden_nodes = 100\n",
    "hidden_nodes2 = 50\n",
    "\n",
    "train =train.values.tolist()\n",
    "test = test.values.tolist()\n",
    "for x in range(len(train)):\n",
    "    train[x][-1] = int(train[x][-1]) \n",
    "    \n",
    "\n",
    "for y in range(len(test)):\n",
    "    test[y][-1] = int(test[y][-1])\n",
    "\n",
    "actual = [test[row][-1] for row in range(len(test))]\n",
    "    \n",
    "for row in range(len(test)):\n",
    "    test[row][-1] = None\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test)):\n",
    "    #assign label to None\n",
    "    test[row][-1] = None\n",
    "#get the predicted labels\n",
    "y_pred =[]\n",
    "y_pred = train_NN2(train, test, learning, epoch, hidden_nodes,hidden_nodes2)\n",
    "\n",
    "#get the accuracy of the model\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hd_BSjoCwVQQ"
   },
   "outputs": [],
   "source": [
    "#PART ii\n",
    "#set learning rate, number of epochs and number of hidden layers in model\n",
    "learning = 0.1\n",
    "epoch = 10\n",
    "hidden_nodes = 124\n",
    "hidden_nodes2 = 50\n",
    "train =[]\n",
    "test = []\n",
    "#convert the dataframes to list \n",
    "train =df_train.values.tolist()\n",
    "test = df_test.values.tolist()\n",
    "\n",
    "#for the training and test set convert the label of each row to an integer from a float\n",
    "for x in range(len(train)):\n",
    "    train[x][-1] = int(train[x][-1]) \n",
    "\n",
    "for y in range(len(test)):\n",
    "    test[y][-1] = int(test[y][-1])\n",
    "\n",
    "#get the actual output label from the test set\n",
    "y_test = []\n",
    "for row in range(len(test)):\n",
    "    y_test.append(test[row][-1])\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test)):\n",
    "    #assign label to None\n",
    "    test[row][-1] = None\n",
    "#get the predicted labels\n",
    "ypred1 = []\n",
    "y_pred1 = train_NN2(train, test, learning, epoch, hidden_nodes, hidden_nodes2)\n",
    "\n",
    "#get the accuracy of the model\n",
    "accuracy_score(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLw0IsdLwVQT"
   },
   "source": [
    "# Part 4b:\n",
    "\n",
    "\n",
    "For 2nd enhancement I compared the accuracy of the Neural Network using different activation functions to calculate weights, different from our original activation, which was tanh(x). \n",
    "The activation functions I used were:\n",
    "- Sigmoid Function\n",
    "- ArcTan\n",
    "\n",
    "References :\n",
    "- https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCd3N0ArwVQU"
   },
   "source": [
    "# Using Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3169D3_wVQV"
   },
   "outputs": [],
   "source": [
    "#Network using Sigmoid Function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import random\n",
    "import math\n",
    "\n",
    "    \n",
    "#function to create initial Neural Network \n",
    "#takes in parameters for inputs, hidden nodes and outputs\n",
    "def init_NN(nodes_in, nodes_h, nodes_out):\n",
    "    \n",
    "    #create list for all initalised nodes and their weights\n",
    "    init = []\n",
    "    #create list for hidden nodes\n",
    "    layer_h = []\n",
    "    #create list for output nodes\n",
    "    layer_out = []\n",
    "    \n",
    "    #for all nodes in hidden append random weight to dict for each input node +1\n",
    "    #there will be extra input weight for the bias\n",
    "    for i in range(nodes_h):\n",
    "        l = []\n",
    "        for j in range(nodes_in+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_h.append(dict_weights)\n",
    "        \n",
    "    #for all nodes in output append random weight to dict for each hidden node +1\n",
    "    #there will be extra hidden weight for the bias\n",
    "    for i in range(nodes_out):\n",
    "        l = []\n",
    "        for j in range(nodes_h+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_out.append(dict_weights)\n",
    "        \n",
    "    #append both hidden and output lists to initalised Neural Network\n",
    "    init.append(layer_h)\n",
    "    init.append(layer_out)\n",
    "    return init\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "#sigmoid derivative activate weights function for gradiant descent \n",
    "def derivative_sigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "#function to forward propagate through network\n",
    "def forward_prop(init, inputs):\n",
    "    for layer in init:\n",
    "        #new list to update weights\n",
    "        updated_weights = []\n",
    "        \n",
    "        for node in layer:\n",
    "            #get the weights from the dictionary for eachnode\n",
    "            weights = node['w']\n",
    "            #inialised to last weight in dict\n",
    "            initalise = weights[-1]\n",
    "            #add new weights  = old weights  * input data\n",
    "            for i in range(len(weights)-1):\n",
    "                initalise += weights[i] * inputs[i]\n",
    "            #use tanh activation on new weights as non-linear model\n",
    "            node['o'] = sigmoid(initalise)\n",
    "            #output this as inputs for next layer\n",
    "            updated_weights.append(node['o'])\n",
    "        inputs = updated_weights\n",
    "    return inputs\n",
    "\n",
    "#cost function to determine the error of each node for gradient descent \n",
    "def cost(init, output):\n",
    "    for row in reversed(range(len(init))):\n",
    "        layer = init[row]\n",
    "        cost = []\n",
    "        #for nodes except the output find the cost of that node\n",
    "        if row != len(init)-1:\n",
    "            for cell in range(len(layer)):\n",
    "                #inialise cost to zero\n",
    "                node_cost = 0.0\n",
    "                #get the cost of each node in layer by finding error\n",
    "                #multiply delta value of node with the weight of that node\n",
    "                for node in init[row + 1]:\n",
    "                    node_cost += (node['w'][cell] * node['d'])\n",
    "                cost.append(node_cost)\n",
    "        else:\n",
    "            #for output node cost is predicted output - actual value\n",
    "            for cell in range(len(layer)):\n",
    "                node = layer[cell]\n",
    "                cost.append(output[cell] - node['o'])\n",
    "        for cell in range(len(layer)):\n",
    "            #get the cost for each node node multiplying error by the derivative of activation\n",
    "            #tanh on the output node\n",
    "            #assign this to delta in node which will be used to update weights\n",
    "            node = layer[cell]\n",
    "            node['d'] = cost[cell] * derivative_sigmoid(node['o'])\n",
    "            \n",
    "#back propagte through the neural network updating the weights with SGD\n",
    "def back_prop(init, data, learning, epoch, output):\n",
    "    #repeat feed forward and backwards for each epoch \n",
    "    for e in range(epoch):\n",
    "        for inputs in data:\n",
    "            #for every row in training set call forward propagate \n",
    "            result = forward_prop(init, inputs)\n",
    "            #give the label ouputs as parameters for find the cost of nodes\n",
    "            outputs = [0 for out in range(output)]\n",
    "            outputs[inputs[-1]] = 1\n",
    "            cost(init, outputs)\n",
    "            #update weights based on new cost\n",
    "            for layer in range(len(init)):\n",
    "                update_input = inputs[:-1]\n",
    "                #for all layers add output result to list\n",
    "                if layer !=0:\n",
    "                    update_input = []\n",
    "                    for node in init[layer-1]:\n",
    "                        update_input.append(node['o'])\n",
    "                #for every node in each layer change the weights based on formula SGD\n",
    "                # ( learning rate * cost of error of that node * the output of node )\n",
    "                for node in init[layer]:\n",
    "                    for cell in range(len(update_input)):\n",
    "                        node['w'][cell] += learning*node['d']*update_input[cell]\n",
    "                        \n",
    "#method to predict the y values of each row in test returning max from forward propagation of the model               \n",
    "def predict(init, row):\n",
    "    out = forward_prop(init, row)\n",
    "    return out.index(max(out))\n",
    "\n",
    "#method to train and test the Neural Network\n",
    "def train_NN(train, test, learning, epoch, hidden):\n",
    "    #set the amount of inputs to NN by the length of columns for parameters\n",
    "    inputs = len(train[0]) - 1\n",
    "    #set amount of outputs to all values outputted ie. 0/1\n",
    "    outputs = len(set([row[-1] for row in train]))\n",
    "    #call init_NN function to set amount of nodes in each layer and their weights\n",
    "    init = init_NN(inputs, hidden,outputs)\n",
    "    #call back propagate function to train model\n",
    "    back_prop(init, train, learning, epoch, outputs)\n",
    "    #create list of predictions for each row of test data\n",
    "    y_pred = []\n",
    "    for row in test:\n",
    "        row_predict = predict(init, row)\n",
    "        y_pred.append(row_predict)\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1G7E2G7qwVQX"
   },
   "outputs": [],
   "source": [
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data_dict = unpickle(\"data_batch_1\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data1 = data_dict[b'data']\n",
    "\n",
    "\"\"\" This function converts the raw array of 3072 floats describing each image into a structure where the colour of each pixel\n",
    "in an image is represented by its r,g,b (in this order) value for every pixel in the image in question.\n",
    "The input into the function is an array of 3072 floats.\n",
    "This assumes that the raw data coming from the CIFAR data is in the format where all the r values come first, followed by\n",
    "all the b values and then all the g values \"\"\"\n",
    "def convert_rgb(dataplusindex):\n",
    "    rgb_picture = dataplusindex\n",
    "    rgb_picture.shape = (3,32,32)\n",
    "    rgb_picture = rgb_picture.transpose([1, 2, 0])\n",
    "   \n",
    "    return rgb_picture\n",
    "\n",
    "\n",
    "\"\"\"This function converts the R,G,B pixels into greyscale pixels. There are a few methods for doing this.\n",
    "If the data is ordered correctly as R,G,B it is appropriate to use Method 2 (link below) which multiplies each component colour by\n",
    "a certain constant, and then sums the total.\n",
    "If not, one can use a standard average of the R,G,B, Method 1 (link below)\"\"\"    \n",
    "def convert_greyscale(rgb_pic):\n",
    "    greyscale_array = []\n",
    "    for element in rgb_pic:\n",
    "        for rgb_row in element:\n",
    "            x = round(np.dot(rgb_row,[0.299, 0.587, 0.114]), 2) #https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 2, assuming the rgb values come in in the format r, g, b and not b,g,r for example\n",
    "            #x = (rgb_row[0] + rgb_row[1] + rgb_row[2])/3 # https://www.prasannakumarr.in/journal/color-to-grayscale-python-image-processing. Method 1\n",
    "            greyscale_array.append(x)\n",
    "    greyscale_array = np.array(greyscale_array)\n",
    "    return greyscale_array\n",
    "\n",
    "# Converting the data into format [greyscale list, classification].\n",
    "# Each data instance will be added to a larger list.\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list = []\n",
    "for i in range(len(data_dict[b'labels'])):\n",
    "    if data_dict[b'labels'][i] == 4 or data_dict[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr = convert_greyscale(rgb_pic)\n",
    "        if data_dict[b'labels'][i] == 4:\n",
    "            greyscale_arr = np.append(greyscale_arr,0)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "           \n",
    "        if data_dict[b'labels'][i] == 6:\n",
    "            greyscale_arr = np.append(greyscale_arr,1)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list = []\n",
    "for element in Frog_Deer_classified_list:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_train = pd.DataFrame(new_Frog_Deer_classified_list)\n",
    "#print(df_train)\n",
    "\n",
    "\n",
    "\n",
    "data_dict_test = unpickle(\"test_batch\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data_test = data_dict_test[b'data']\n",
    "\n",
    "# Converting the data into format [greyscale list, classification].\n",
    "# Each data instance will be added to a larger list.\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list_test = []\n",
    "for i in range(len(data_dict_test[b'labels'])):\n",
    "    if data_dict_test[b'labels'][i] == 4 or data_dict_test[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic_test = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr_test = convert_greyscale(rgb_pic_test)\n",
    "        if data_dict_test[b'labels'][i] == 4:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,0)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "           \n",
    "        if data_dict_test[b'labels'][i] == 6:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,1)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list_test = []\n",
    "for element in Frog_Deer_classified_list_test:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list_test.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_test = pd.DataFrame(new_Frog_Deer_classified_list_test)\n",
    "#df_test\n",
    "\n",
    "\n",
    "#set learning rate, number of epochs and number of hidden layers in model\n",
    "learning = 0.1\n",
    "epoch = 10\n",
    "hidden_nodes = 50\n",
    "\n",
    "#convert the dataframes to list \n",
    "train_sigmoid =df_train.values.tolist()\n",
    "test_sigmoid = df_test.values.tolist()\n",
    "\n",
    "#for the training and test set convert the label of each row to an integer from a float\n",
    "for x in range(len(train_sigmoid)):\n",
    "    train_sigmoid[x][-1] = int(train_sigmoid[x][-1]) \n",
    "\n",
    "for y in range(len(test_sigmoid)):\n",
    "    test_sigmoid[y][-1] = int(test_sigmoid[y][-1])\n",
    "\n",
    "#get the actual output label from the test set\n",
    "y_test_sigmoid = []\n",
    "for row in range(len(test_sigmoid)):\n",
    "    y_test_sigmoid.append(test_sigmoid[row][-1])\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test_sigmoid)):\n",
    "    #assign label to None\n",
    "    test_sigmoid[row][-1] = None\n",
    "    \n",
    "#get the predicted labels\n",
    "#y_pred_sigmoid = train_NN(train_sigmoid, test_sigmoid, learning, epoch, hidden_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrZ7EVycwVQa"
   },
   "outputs": [],
   "source": [
    "# This piece of code calculates the average accuracy over 10 tests with the Sigmoid function\n",
    "accuracy_scores_sigmoid = []\n",
    "for i in range(10):\n",
    "    y_pred_sigmoid = train_NN(train_sigmoid, test_sigmoid, learning, epoch, hidden_nodes) \n",
    "    #get the accuracy of the model\n",
    "    x = accuracy_score(y_test_sigmoid, y_pred_sigmoid)\n",
    "    accuracy_scores_sigmoid.append(x)\n",
    "            \n",
    "\n",
    "average_score = (sum(accuracy_scores_sigmoid)/len(accuracy_scores_sigmoid))\n",
    "print(\"Average network accuracy over 10 iterations with Sigmoid\", average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2oDK0yOwVQd"
   },
   "source": [
    "# Using ArcTan Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N25YORHFwVQf"
   },
   "outputs": [],
   "source": [
    "#Network using ArcTan Function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import random\n",
    "import math as m\n",
    "\n",
    "\n",
    "#function to create initial Neural Network \n",
    "#takes in parameters for inputs, hidden nodes and outputs\n",
    "def init_NN(nodes_in, nodes_h, nodes_out):\n",
    "    \n",
    "    #create list for all initalised nodes and their weights\n",
    "    init = []\n",
    "    #create list for hidden nodes\n",
    "    layer_h = []\n",
    "    #create list for output nodes\n",
    "    layer_out = []\n",
    "    \n",
    "    #for all nodes in hidden append random weight to dict for each input node +1\n",
    "    #there will be extra input weight for the bias\n",
    "    for i in range(nodes_h):\n",
    "        l = []\n",
    "        for j in range(nodes_in+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_h.append(dict_weights)\n",
    "        \n",
    "    #for all nodes in output append random weight to dict for each hidden node +1\n",
    "    #there will be extra hidden weight for the bias\n",
    "    for i in range(nodes_out):\n",
    "        l = []\n",
    "        for j in range(nodes_h+1):\n",
    "            l.append(random())\n",
    "        dict_weights = {\"w\":l}\n",
    "        layer_out.append(dict_weights)\n",
    "        \n",
    "    #append both hidden and output lists to initalised Neural Network\n",
    "    init.append(layer_h)\n",
    "    init.append(layer_out)\n",
    "    return init\n",
    "\n",
    "def ArcTan(x):\n",
    "    return math.atan(x)\n",
    "\n",
    "def derivative_ArcTan(x):\n",
    "    return 1/(x**2 + 1)\n",
    "\n",
    "#function to forward propagate through network\n",
    "def forward_prop(init, inputs):\n",
    "    for layer in init:\n",
    "        #new list to update weights\n",
    "        updated_weights = []\n",
    "        \n",
    "        for node in layer:\n",
    "            #get the weights from the dictionary for eachnode\n",
    "            weights = node['w']\n",
    "            #inialised to last weight in dict\n",
    "            initalise = weights[-1]\n",
    "            #add new weights  = old weights  * input data\n",
    "            for i in range(len(weights)-1):\n",
    "                initalise += weights[i] * inputs[i]\n",
    "            #use tanh activation on new weights as non-linear model\n",
    "            node['o'] = ArcTan(initalise)\n",
    "            #output this as inputs for next layer\n",
    "            updated_weights.append(node['o'])\n",
    "        inputs = updated_weights\n",
    "    return inputs\n",
    "\n",
    "#cost function to determine the error of each node for gradient descent \n",
    "def cost(init, output):\n",
    "    for row in reversed(range(len(init))):\n",
    "        layer = init[row]\n",
    "        cost = []\n",
    "        #for nodes except the output find the cost of that node\n",
    "        if row != len(init)-1:\n",
    "            for cell in range(len(layer)):\n",
    "                #inialise cost to zero\n",
    "                node_cost = 0.0\n",
    "                #get the cost of each node in layer by finding error\n",
    "                #multiply delta value of node with the weight of that node\n",
    "                for node in init[row + 1]:\n",
    "                    node_cost += (node['w'][cell] * node['d'])\n",
    "                cost.append(node_cost)\n",
    "        else:\n",
    "            #for output node cost is predicted output - actual value\n",
    "            for cell in range(len(layer)):\n",
    "                node = layer[cell]\n",
    "                cost.append(output[cell] - node['o'])\n",
    "        for cell in range(len(layer)):\n",
    "            #get the cost for each node node multiplying error by the derivative of activation\n",
    "            #tanh on the output node\n",
    "            #assign this to delta in node which will be used to update weights\n",
    "            node = layer[cell]\n",
    "            node['d'] = cost[cell] * derivative_ArcTan(node['o'])\n",
    "            \n",
    "#back propagte through the neural network updating the weights with SGD\n",
    "def back_prop(init, data, learning, epoch, output):\n",
    "    #repeat feed forward and backwards for each epoch \n",
    "    for e in range(epoch):\n",
    "        for inputs in data:\n",
    "            #for every row in training set call forward propagate \n",
    "            result = forward_prop(init, inputs)\n",
    "            #give the label ouputs as parameters for find the cost of nodes\n",
    "            outputs = [0 for out in range(output)]\n",
    "            outputs[inputs[-1]] = 1\n",
    "            cost(init, outputs)\n",
    "            #update weights based on new cost\n",
    "            for layer in range(len(init)):\n",
    "                update_input = inputs[:-1]\n",
    "                #for all layers add output result to list\n",
    "                if layer !=0:\n",
    "                    update_input = []\n",
    "                    for node in init[layer-1]:\n",
    "                        update_input.append(node['o'])\n",
    "                #for every node in each layer change the weights based on formula SGD\n",
    "                # ( learning rate * cost of error of that node * the output of node )\n",
    "                for node in init[layer]:\n",
    "                    for cell in range(len(update_input)):\n",
    "                        node['w'][cell] += learning*node['d']*update_input[cell]\n",
    "                        \n",
    "#method to predict the y values of each row in test returning max from forward propagation of the model               \n",
    "def predict(init, row):\n",
    "    out = forward_prop(init, row)\n",
    "    return out.index(max(out))\n",
    "\n",
    "#method to train and test the Neural Network\n",
    "def train_NN(train, test, learning, epoch, hidden):\n",
    "    #set the amount of inputs to NN by the length of columns for parameters\n",
    "    inputs = len(train[0]) - 1\n",
    "    #set amount of outputs to all values outputted ie. 0/1\n",
    "    outputs = len(set([row[-1] for row in train]))\n",
    "    #call init_NN function to set amount of nodes in each layer and their weights\n",
    "    init = init_NN(inputs, hidden,outputs)\n",
    "    #call back propagate function to train model\n",
    "    back_prop(init, train, learning, epoch, outputs)\n",
    "    #create list of predictions for each row of test data\n",
    "    y_pred = []\n",
    "    for row in test:\n",
    "        row_predict = predict(init, row)\n",
    "        y_pred.append(row_predict)\n",
    "    return(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WViGgUHowVQi"
   },
   "outputs": [],
   "source": [
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data_dict = unpickle(\"data_batch_1\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data1 = data_dict[b'data']\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list = []\n",
    "for i in range(len(data_dict[b'labels'])):\n",
    "    if data_dict[b'labels'][i] == 4 or data_dict[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr = convert_greyscale(rgb_pic)\n",
    "        if data_dict[b'labels'][i] == 4:\n",
    "            greyscale_arr = np.append(greyscale_arr,0)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "           \n",
    "        if data_dict[b'labels'][i] == 6:\n",
    "            greyscale_arr = np.append(greyscale_arr,1)\n",
    "            Frog_Deer_classified_list.append(greyscale_arr)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list = []\n",
    "for element in Frog_Deer_classified_list:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_train = pd.DataFrame(new_Frog_Deer_classified_list)\n",
    "#print(df_train)\n",
    "\n",
    "\n",
    "\n",
    "data_dict_test = unpickle(\"test_batch\") # Keys: {b'batch_label', b'labels', b'data', b'filenames' }\n",
    "\n",
    "data_test = data_dict_test[b'data']\n",
    "\n",
    "# Converting the data into format [greyscale list, classification].\n",
    "# Each data instance will be added to a larger list.\n",
    "\n",
    "#Frog is class 6 in the CIFAR data\n",
    "#Deer is class 4 in the CIFAR data\n",
    "\n",
    "# Choosing Frog to be class 1 for our implementation\n",
    "# Choosing Deer to be class 0 for our implementation\n",
    "\n",
    "Frog_Deer_classified_list_test = []\n",
    "for i in range(len(data_dict_test[b'labels'])):\n",
    "    if data_dict_test[b'labels'][i] == 4 or data_dict_test[b'labels'][i] ==6: #Filtering for Frog and Deer data, CIFAR classes 4 and 6\n",
    "        rgb_pic_test = convert_rgb(data_dict[b'data'][i])\n",
    "        greyscale_arr_test = convert_greyscale(rgb_pic_test)\n",
    "        if data_dict_test[b'labels'][i] == 4:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,0)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "           \n",
    "        if data_dict_test[b'labels'][i] == 6:\n",
    "            greyscale_arr_test = np.append(greyscale_arr_test,1)\n",
    "            Frog_Deer_classified_list_test.append(greyscale_arr_test)\n",
    "\n",
    "\n",
    "#Now converting list from numpy array to list for inputting into pandas dataframe\n",
    "new_Frog_Deer_classified_list_test = []\n",
    "for element in Frog_Deer_classified_list_test:\n",
    "    x = element.tolist()\n",
    "    new_Frog_Deer_classified_list_test.append(x)\n",
    "   \n",
    "# Creating Pandas Data Frame from the filtered data\n",
    "# Each row of the data frame contains 1025 entries (1025 columns), these are 1024 greyscale pixels and a classification (1/Frog or 0/Deer)\n",
    "df_test = pd.DataFrame(new_Frog_Deer_classified_list_test)\n",
    "#df_test\n",
    "\n",
    "\n",
    "#set learning rate, number of epochs and number of hidden layers in model\n",
    "learning = 0.1\n",
    "epoch = 10\n",
    "hidden_nodes = 50\n",
    "\n",
    "#convert the dataframes to list \n",
    "train_ArcTan =df_train.values.tolist()\n",
    "test_ArcTan = df_test.values.tolist()\n",
    "\n",
    "#for the training and test set convert the label of each row to an integer from a float\n",
    "for x in range(len(train_ArcTan)):\n",
    "    train_ArcTan[x][-1] = int(train_ArcTan[x][-1]) \n",
    "\n",
    "for y in range(len(test_ArcTan)):\n",
    "    test_ArcTan[y][-1] = int(test_ArcTan[y][-1])\n",
    "\n",
    "#get the actual output label from the test set\n",
    "y_test_ArcTan = []\n",
    "for row in range(len(test_ArcTan)):\n",
    "    y_test_ArcTan.append(test_ArcTan[row][-1])\n",
    "\n",
    "#remove the label for test set so can relabel with prediction\n",
    "for row in range(len(test_ArcTan)):\n",
    "    #assign label to None\n",
    "    test_ArcTan[row][-1] = None\n",
    "    \n",
    "#get the predicted labels\n",
    "#y_pred_sigmoid = train_NN(train_sigmoid, test_sigmoid, learning, epoch, hidden_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code calculates the average accuracy over 10 tests with the ArcTan function\n",
    "accuracy_scores_ArcTan = []\n",
    "for i in range(10):\n",
    "    y_pred_ArcTan = train_NN(train_ArcTan, test_ArcTan, learning, epoch, hidden_nodes) \n",
    "    #get the accuracy of the model\n",
    "    x = accuracy_score(y_test_ArcTan, y_pred_ArcTan)\n",
    "    accuracy_scores_ArcTan.append(x)\n",
    "            \n",
    "\n",
    "average_score = (sum(accuracy_scores_ArcTan)/len(accuracy_scores_ArcTan))\n",
    "print(\"Average network accuracy over 10 iterations with ArcTan\", average_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment1_deeplearningv2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
